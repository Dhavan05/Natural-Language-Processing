{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "BO-HU-7uorVX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shadow/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/shadow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/shadow/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/shadow/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Clean data funtion: #\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "#string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "  # From the last assignment\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www.\\S+\", \"\", text)\n",
    "    text_links_removed = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text_cleaned = \" \".join([word for word in re.split('\\W+', text_links_removed)\n",
    "        if word not in stopword])\n",
    "    text = \" \".join([wn.lemmatize(word) for word in re.split('\\W+', text_cleaned)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni4T_dwEHFVa"
   },
   "source": [
    "## CS 6120: Natural Language Processing - Prof. Ahmad Uzair\n",
    "\n",
    "### Assignment 2: Text Classification and Neural Network\n",
    "### Total Points: 100 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loQ22M-bCubq"
   },
   "source": [
    "In Assignment 2, you will be dealing with text classification using Multinomial Naive Bayes and Neural Networks. You will also be dealing with vector visualization. In the previous assingment you implemented Bag of Words as the feature selection method. However, in this assignment you will be using TF-IDF Vectorization instead of Bag of Words. We recommend starting with this assignment a little early as the datasets are quite large and several parts of the assignment might take long duration to execute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3brglC1C0mZ"
   },
   "source": [
    "## Question 1 Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ1y75IoC3rE"
   },
   "source": [
    "In the first question you will be dealing with 20 News Group Dataset. You are required to implement TF-IDF vectorization from scratch and perform Multinomial Naive Bayes Classification on the News Group Dataset.\n",
    "You may use appropriate packages or modules for fitting the Multinomial Naive Bayes Model, however, the implementation of the TF-IDF Vectorization should be from the scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRjM45PyC8ML"
   },
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "Link to the original dataset: http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
    "\n",
    "You can also import the dataset from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "CmFf2INNDJRb"
   },
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize \n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "AEp1SHe5DKCB"
   },
   "outputs": [],
   "source": [
    "# Import the 20 news group dataset utilizing sklearn library\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "mydata_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "mydata_test =  fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "OscOmcE0DMHa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Print the news groups(target) in the dataset\n",
    "pprint(list(mydata_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "JvQb2r0aDR_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "# What is the type of 'mydata_train' and 'mydata_test'\n",
    "print(type(mydata_train))\n",
    "print(type(mydata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "ozzwyREhDaMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "11314\n",
      "7532\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the data\n",
    "\n",
    "print(len(mydata_train.data))\n",
    "print(len(mydata_train.filenames))\n",
    "print(len(mydata_test.data))\n",
    "print(len(mydata_test.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlipMuEpDz-K"
   },
   "source": [
    "### Expected Output: \n",
    "11314\n",
    "\n",
    "11314\n",
    "\n",
    "7532\n",
    "\n",
    "7532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55FKOBBuEDI2"
   },
   "source": [
    "## Extracting Features from the Dataset                        (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4GDENzmEEkG"
   },
   "source": [
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgxfDXmxEHid"
   },
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qNFFYyEKsa"
   },
   "source": [
    "Our model cannot simply read the text data so we convert it into numerical format. In order to convert the data into numerical format we create vectors from text.\n",
    "\n",
    "For this particular purpose we could either employ Bag of Words or TF-IDF Vectorization\n",
    "\n",
    "Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency, which instead of giving more weight to words that occur more frequently, it gives a higher weight to words that occur less frequently.\n",
    "\n",
    "Ref:https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/#:~:text=Bag%20of%20Words%20just%20creates,less%20important%20ones%20as%20well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLzgRJRZEP3k"
   },
   "source": [
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe5lHi3NE1QJ"
   },
   "source": [
    "Term Frequency is the measure of the frequency of words in a document. It is the ratio of the number of times the word appears in a document compared to the total number of words in that document.\n",
    "\n",
    "The words that occur rarely in the corpus have a high IDF score. It is the log of the ratio of the number of documents to the number of documents containing the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXotAER_EQ8T"
   },
   "source": [
    "idf(t) = log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "2vzVI8ylFAEl"
   },
   "outputs": [],
   "source": [
    "text = mydata_train.data\n",
    "test = mydata_test.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2AatmvFtE-"
   },
   "source": [
    "## Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "XyJbe42AFuHp"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "lines = [] \n",
    "word_list = []\n",
    "\n",
    "for line in text:\n",
    "    #tokenize the text documents and update the lists word_list and lines\n",
    "    words = clean_text(line).split()\n",
    "    word_list = word_list + words\n",
    "    lines.append(words)\n",
    "\n",
    "# Make sure the word_list contains unique tokens\n",
    "word_list = np.unique(word_list)\n",
    "\n",
    "# Calculate the total documents present in the corpus\n",
    "total_docs = len(lines)\n",
    " \n",
    "#Create a dictionary to keep track of index of each word\n",
    "dict_idx = {}\n",
    "counter = 0\n",
    "for word in word_list:\n",
    "    dict_idx[word] = counter\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "E6M_CoI9FxYb"
   },
   "outputs": [],
   "source": [
    "# Create a frequency dictionary\n",
    "def frequency_dict(lines):\n",
    "    '''\n",
    "    lines: list containing all the tokens\n",
    "    ---\n",
    "    freq_word: returns a dictionary which keeps the count of the number of documents containing the given word\n",
    "    '''\n",
    "    freq_word = {}\n",
    "    for line in lines:\n",
    "        unique_words = np.unique(line)\n",
    "        for word in unique_words:\n",
    "            if word in freq_word:\n",
    "                freq_word[word] = freq_word[word] + 1\n",
    "            else:\n",
    "                freq_word[word] = 1\n",
    "                \n",
    "    return freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "qFkt9KBgFz43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2door': 3,\n",
       " '60': 198,\n",
       " '70': 121,\n",
       " 'addition': 171,\n",
       " 'anyone': 1376,\n",
       " 'body': 285,\n",
       " 'bricklin': 2,\n",
       " 'bumper': 22,\n",
       " 'called': 566,\n",
       " 'car': 482,\n",
       " 'could': 1674,\n",
       " 'day': 845,\n",
       " 'door': 142,\n",
       " 'early': 268,\n",
       " 'email': 836,\n",
       " 'engine': 164,\n",
       " 'enlighten': 13,\n",
       " 'front': 232,\n",
       " 'funky': 5,\n",
       " 'history': 298,\n",
       " 'info': 343,\n",
       " 'know': 2409,\n",
       " 'late': 170,\n",
       " 'looked': 194,\n",
       " 'looking': 570,\n",
       " 'made': 808,\n",
       " 'model': 306,\n",
       " 'name': 600,\n",
       " 'please': 1160,\n",
       " 'production': 71,\n",
       " 'really': 1074,\n",
       " 'rest': 324,\n",
       " 'saw': 265,\n",
       " 'separate': 133,\n",
       " 'small': 432,\n",
       " 'spec': 106,\n",
       " 'sport': 96,\n",
       " 'tellme': 1,\n",
       " 'whatever': 293,\n",
       " 'wondering': 181,\n",
       " 'year': 1418,\n",
       " '14': 281,\n",
       " '800': 78,\n",
       " 'adapter': 73,\n",
       " 'add': 296,\n",
       " 'answered': 69,\n",
       " 'attained': 7,\n",
       " 'base': 205,\n",
       " 'brave': 67,\n",
       " 'brief': 51,\n",
       " 'card': 517,\n",
       " 'clock': 97,\n",
       " 'cpu': 91,\n",
       " 'detailing': 11,\n",
       " 'disk': 326,\n",
       " 'done': 579,\n",
       " 'especially': 327,\n",
       " 'experience': 351,\n",
       " 'fair': 117,\n",
       " 'floppy': 137,\n",
       " 'functionality': 36,\n",
       " 'havent': 286,\n",
       " 'heat': 82,\n",
       " 'hour': 199,\n",
       " 'knowledge': 204,\n",
       " 'message': 373,\n",
       " 'network': 213,\n",
       " 'next': 512,\n",
       " 'number': 848,\n",
       " 'oscillator': 19,\n",
       " 'per': 244,\n",
       " 'poll': 31,\n",
       " 'procedure': 87,\n",
       " 'rated': 32,\n",
       " 'requested': 45,\n",
       " 'send': 479,\n",
       " 'shared': 68,\n",
       " 'si': 32,\n",
       " 'sink': 25,\n",
       " 'soul': 88,\n",
       " 'speed': 339,\n",
       " 'summarizing': 9,\n",
       " 'thanks': 1169,\n",
       " 'top': 275,\n",
       " 'two': 1273,\n",
       " 'upgrade': 110,\n",
       " 'upgraded': 34,\n",
       " 'usage': 52,\n",
       " '120': 68,\n",
       " '160': 34,\n",
       " '180': 20,\n",
       " '185c': 1,\n",
       " '1985': 44,\n",
       " '512k': 16,\n",
       " '80mb': 11,\n",
       " 'access': 300,\n",
       " 'active': 92,\n",
       " 'actually': 683,\n",
       " 'advance': 367,\n",
       " 'answer': 451,\n",
       " 'anybody': 361,\n",
       " 'anymore': 74,\n",
       " 'appearence': 2,\n",
       " 'around': 773,\n",
       " 'back': 1002,\n",
       " 'better': 845,\n",
       " 'bit': 629,\n",
       " 'breifly': 2,\n",
       " 'bunch': 133,\n",
       " 'computer': 490,\n",
       " 'corner': 83,\n",
       " 'daily': 74,\n",
       " 'daytoday': 8,\n",
       " 'dirt': 30,\n",
       " 'display': 254,\n",
       " 'dont': 2319,\n",
       " 'drop': 137,\n",
       " 'duo': 33,\n",
       " 'electrical': 62,\n",
       " 'engineering': 104,\n",
       " 'expected': 150,\n",
       " 'feel': 418,\n",
       " 'figured': 43,\n",
       " 'final': 200,\n",
       " 'finally': 228,\n",
       " 'folk': 202,\n",
       " 'gave': 209,\n",
       " 'get': 2332,\n",
       " 'ghost': 15,\n",
       " 'good': 1583,\n",
       " 'got': 859,\n",
       " 'great': 640,\n",
       " 'heard': 484,\n",
       " 'hellcat': 2,\n",
       " 'helpful': 100,\n",
       " 'hit': 244,\n",
       " 'hopefully': 87,\n",
       " 'id': 708,\n",
       " 'ill': 497,\n",
       " 'im': 1652,\n",
       " 'impression': 67,\n",
       " 'intended': 156,\n",
       " 'introduction': 59,\n",
       " 'ive': 924,\n",
       " 'life': 566,\n",
       " 'like': 2536,\n",
       " 'line': 668,\n",
       " 'look': 986,\n",
       " 'mac': 263,\n",
       " 'machine': 402,\n",
       " 'macleak': 2,\n",
       " 'make': 1701,\n",
       " 'market': 192,\n",
       " 'maybe': 557,\n",
       " 'might': 903,\n",
       " 'money': 371,\n",
       " 'much': 1452,\n",
       " 'new': 1373,\n",
       " 'news': 326,\n",
       " 'one': 3252,\n",
       " 'opinion': 385,\n",
       " 'people': 1821,\n",
       " 'perform': 80,\n",
       " 'picking': 30,\n",
       " 'played': 137,\n",
       " 'plus': 240,\n",
       " 'post': 664,\n",
       " 'powerbook': 31,\n",
       " 'premium': 22,\n",
       " 'price': 443,\n",
       " 'probably': 795,\n",
       " 'prove': 158,\n",
       " 'purdue': 7,\n",
       " 'question': 1042,\n",
       " 'rather': 623,\n",
       " 'reading': 296,\n",
       " 'real': 605,\n",
       " 'realize': 158,\n",
       " 'recently': 286,\n",
       " 'round': 125,\n",
       " 'rumor': 57,\n",
       " 'since': 1082,\n",
       " 'size': 266,\n",
       " 'solicit': 5,\n",
       " 'somebody': 160,\n",
       " 'sooner': 30,\n",
       " 'sooo': 4,\n",
       " 'starting': 150,\n",
       " 'store': 132,\n",
       " 'subjective': 32,\n",
       " 'summary': 122,\n",
       " 'summer': 116,\n",
       " 'supposed': 194,\n",
       " 'swing': 24,\n",
       " 'taking': 257,\n",
       " 'time': 1970,\n",
       " 'tom': 109,\n",
       " 'twillisecnpurdueedu': 1,\n",
       " 'us': 302,\n",
       " 'use': 1641,\n",
       " 'way': 1471,\n",
       " 'weekend': 63,\n",
       " 'well': 1628,\n",
       " 'went': 296,\n",
       " 'whats': 243,\n",
       " 'willis': 9,\n",
       " 'worth': 260,\n",
       " 'wow': 36,\n",
       " 'yea': 13,\n",
       " 'addressphone': 3,\n",
       " 'chip': 377,\n",
       " 'information': 676,\n",
       " 'weiteks': 1,\n",
       " '213': 25,\n",
       " 'arent': 280,\n",
       " 'article': 495,\n",
       " 'baker': 16,\n",
       " 'basically': 176,\n",
       " 'bug': 91,\n",
       " 'c5owcbn3pworldstdcom': 1,\n",
       " 'checked': 86,\n",
       " 'code': 347,\n",
       " 'crew': 41,\n",
       " 'error': 268,\n",
       " 'fix': 140,\n",
       " 'ignore': 84,\n",
       " 'introduce': 22,\n",
       " 'known': 338,\n",
       " 'launch': 100,\n",
       " 'liftoff': 9,\n",
       " 'ok': 295,\n",
       " 'possibly': 187,\n",
       " 'right': 1263,\n",
       " 'see': 1379,\n",
       " 'set': 650,\n",
       " 'software': 448,\n",
       " 'suchlike': 4,\n",
       " 'system': 1096,\n",
       " 'tell': 767,\n",
       " 'thing': 1471,\n",
       " 'till': 49,\n",
       " 'tombakerworldstdcom': 1,\n",
       " 'understanding': 147,\n",
       " 'value': 312,\n",
       " 'warning': 98,\n",
       " 'yet': 536,\n",
       " 'allegedly': 16,\n",
       " 'allowed': 167,\n",
       " 'analysis': 117,\n",
       " 'another': 882,\n",
       " 'argument': 331,\n",
       " 'bill': 240,\n",
       " 'class': 189,\n",
       " 'commonly': 51,\n",
       " 'consider': 341,\n",
       " 'course': 731,\n",
       " 'defined': 96,\n",
       " 'destruction': 40,\n",
       " 'doubt': 233,\n",
       " 'evidently': 15,\n",
       " 'first': 1302,\n",
       " 'given': 492,\n",
       " 'later': 374,\n",
       " 'mass': 141,\n",
       " 'must': 812,\n",
       " 'point': 1016,\n",
       " 'presenting': 22,\n",
       " 'quote': 179,\n",
       " 'read': 780,\n",
       " 'rigidly': 2,\n",
       " 'show': 509,\n",
       " 'switching': 38,\n",
       " 'term': 309,\n",
       " 'topic': 144,\n",
       " 'understood': 69,\n",
       " 'using': 957,\n",
       " 'weapon': 187,\n",
       " 'accidentally': 17,\n",
       " 'astrocytomas': 1,\n",
       " 'couldnt': 240,\n",
       " 'debra': 1,\n",
       " 'delete': 30,\n",
       " 'directly': 208,\n",
       " 'everyone': 340,\n",
       " 'file': 613,\n",
       " 'glad': 86,\n",
       " 'hmmm': 60,\n",
       " 'instead': 383,\n",
       " 'last': 915,\n",
       " 'mailbouncing': 1,\n",
       " 'probs': 5,\n",
       " 'publicly': 57,\n",
       " 'request': 177,\n",
       " 'responded': 54,\n",
       " 'rm': 27,\n",
       " 'rn': 6,\n",
       " 'sean': 18,\n",
       " 'september': 40,\n",
       " 'sharon': 10,\n",
       " 'sure': 891,\n",
       " 'thank': 218,\n",
       " 'thought': 594,\n",
       " 'treatment': 118,\n",
       " 'trying': 559,\n",
       " '05mbs': 4,\n",
       " '10mbs': 8,\n",
       " '1520mbs': 7,\n",
       " '161': 15,\n",
       " '16bitwide': 3,\n",
       " '173': 12,\n",
       " '20': 436,\n",
       " '20mbs': 7,\n",
       " '32bitwide': 5,\n",
       " '364406': 4,\n",
       " '40mbs': 7,\n",
       " '46mbs': 8,\n",
       " '5mbs': 7,\n",
       " '6mbs': 2,\n",
       " '812mbs': 7,\n",
       " '83': 43,\n",
       " '8bit': 43,\n",
       " '8bitscsi1': 2,\n",
       " '996': 5,\n",
       " 'although': 364,\n",
       " 'ansynchronous': 1,\n",
       " 'apple': 174,\n",
       " 'available': 578,\n",
       " 'burst': 33,\n",
       " 'controler': 12,\n",
       " 'controller': 157,\n",
       " 'correct': 318,\n",
       " 'data': 447,\n",
       " 'documented': 35,\n",
       " 'driven': 71,\n",
       " 'esdi': 18,\n",
       " 'exist': 236,\n",
       " 'fact': 794,\n",
       " 'fast': 268,\n",
       " 'faster': 163,\n",
       " 'ftp': 207,\n",
       " 'go': 1256,\n",
       " 'ibm': 138,\n",
       " 'ide': 96,\n",
       " 'inconsiant': 1,\n",
       " 'increase': 166,\n",
       " 'indeed': 218,\n",
       " 'infomacreport': 3,\n",
       " 'interface': 150,\n",
       " 'macibmcompareversion': 1,\n",
       " 'maximum': 79,\n",
       " 'may': 1295,\n",
       " 'mode': 251,\n",
       " 'newsgroup': 188,\n",
       " 'note': 491,\n",
       " 'part': 870,\n",
       " 'pc': 281,\n",
       " 'posted': 306,\n",
       " 'problem': 1351,\n",
       " 'quadra': 56,\n",
       " 'range': 164,\n",
       " 'reach': 117,\n",
       " 'said': 851,\n",
       " 'salesperson': 8,\n",
       " 'scsi': 143,\n",
       " 'scsi1': 10,\n",
       " 'scsi2': 18,\n",
       " 'seems': 704,\n",
       " 'sheet': 71,\n",
       " 'slower': 60,\n",
       " 'still': 1020,\n",
       " 'sumexaimstanfordedu': 8,\n",
       " 'synchronous': 13,\n",
       " 'think': 1922,\n",
       " 'though': 699,\n",
       " 'true': 608,\n",
       " 'twice': 99,\n",
       " 'txt': 2,\n",
       " 'version': 458,\n",
       " '30': 367,\n",
       " 'appreciated': 289,\n",
       " 'bmps': 4,\n",
       " 'brando': 1,\n",
       " 'cant': 870,\n",
       " 'change': 523,\n",
       " 'downloaded': 24,\n",
       " 'figure': 264,\n",
       " 'help': 938,\n",
       " 'icon': 52,\n",
       " 'several': 527,\n",
       " 'thanx': 61,\n",
       " 'wallpaper': 13,\n",
       " 'win': 260,\n",
       " 'would': 3163,\n",
       " 'autodoubler': 2,\n",
       " 'autodoublerdiskdoubler': 1,\n",
       " 'board': 282,\n",
       " 'boardcompressed': 1,\n",
       " 'buy': 386,\n",
       " 'competition': 38,\n",
       " 'compression': 64,\n",
       " 'dd': 22,\n",
       " 'decompress': 3,\n",
       " 'diskdoubler': 1,\n",
       " 'due': 312,\n",
       " 'else': 541,\n",
       " 'expand': 40,\n",
       " 'expansion': 87,\n",
       " 'fault': 103,\n",
       " 'fixed': 112,\n",
       " 'freeware': 16,\n",
       " 'hard': 537,\n",
       " 'hey': 141,\n",
       " 'hole': 110,\n",
       " 'however': 800,\n",
       " 'installed': 161,\n",
       " 'licensing': 24,\n",
       " 'lost': 231,\n",
       " 'memory': 323,\n",
       " 'mentioned': 250,\n",
       " 'owner': 176,\n",
       " 'product': 261,\n",
       " 'reappears': 3,\n",
       " 'recompress': 2,\n",
       " 'reference': 302,\n",
       " 'related': 191,\n",
       " 'reluctant': 15,\n",
       " 'sad': 52,\n",
       " 'say': 1439,\n",
       " 'something': 1086,\n",
       " 'stac': 2,\n",
       " 'stacs': 1,\n",
       " 'stinky': 1,\n",
       " 'technology': 239,\n",
       " 'thats': 722,\n",
       " 'theyre': 278,\n",
       " 'troubled': 6,\n",
       " 'unless': 373,\n",
       " 'unlikely': 76,\n",
       " 'usually': 326,\n",
       " 'utility': 106,\n",
       " 'whether': 463,\n",
       " 'without': 861,\n",
       " 'work': 1306,\n",
       " 'writing': 201,\n",
       " 'wrong': 539,\n",
       " '0826': 3,\n",
       " '1': 1171,\n",
       " '17k': 1,\n",
       " '1978': 30,\n",
       " '1st': 93,\n",
       " '3495': 2,\n",
       " '3k': 2,\n",
       " '900gts': 1,\n",
       " 'accel': 4,\n",
       " 'axis': 18,\n",
       " 'beemer': 6,\n",
       " 'bike': 211,\n",
       " 'bronzebrownorange': 1,\n",
       " 'call': 648,\n",
       " 'computracrichardsontx': 3,\n",
       " 'dod': 82,\n",
       " 'ducati': 5,\n",
       " 'faded': 4,\n",
       " 'honk': 4,\n",
       " 'irwin': 4,\n",
       " 'irwincmptrclonestarorg': 3,\n",
       " 'jap': 3,\n",
       " 'leak': 25,\n",
       " 'mate': 6,\n",
       " 'motor': 75,\n",
       " 'nice': 360,\n",
       " 'oil': 87,\n",
       " 'paint': 63,\n",
       " 'pop': 56,\n",
       " 'r756': 3,\n",
       " 'run': 697,\n",
       " 'shop': 68,\n",
       " 'sold': 144,\n",
       " 'stable': 47,\n",
       " 'therefore': 235,\n",
       " 'thinking': 232,\n",
       " 'trans': 13,\n",
       " 'tuba': 3,\n",
       " 'want': 1348,\n",
       " 'abraham': 20,\n",
       " 'absolute': 84,\n",
       " 'analogy': 51,\n",
       " 'ancestor': 18,\n",
       " 'animal': 112,\n",
       " 'anything': 763,\n",
       " 'aside': 100,\n",
       " 'assume': 267,\n",
       " 'attempt': 201,\n",
       " 'bar': 88,\n",
       " 'believe': 855,\n",
       " 'believed': 102,\n",
       " 'bible': 245,\n",
       " 'biblical': 69,\n",
       " 'boundary': 39,\n",
       " 'cart': 11,\n",
       " 'case': 900,\n",
       " 'child': 343,\n",
       " 'christ': 193,\n",
       " 'christian': 383,\n",
       " 'christianity': 130,\n",
       " 'come': 1035,\n",
       " 'comtemporary': 1,\n",
       " 'conclusion': 145,\n",
       " 'considers': 23,\n",
       " 'covenant': 14,\n",
       " 'created': 176,\n",
       " 'david': 271,\n",
       " 'debunk': 5,\n",
       " 'decide': 131,\n",
       " 'different': 651,\n",
       " 'directive': 17,\n",
       " 'disobeys': 1,\n",
       " 'essence': 31,\n",
       " 'establishes': 8,\n",
       " 'even': 1490,\n",
       " 'example': 590,\n",
       " 'faith': 192,\n",
       " 'fall': 191,\n",
       " 'follow': 194,\n",
       " 'follower': 60,\n",
       " 'foundation': 76,\n",
       " 'founded': 36,\n",
       " 'gist': 6,\n",
       " 'guess': 372,\n",
       " 'happy': 175,\n",
       " 'historian': 25,\n",
       " 'hold': 280,\n",
       " 'humanity': 50,\n",
       " 'image': 272,\n",
       " 'inappropriate': 21,\n",
       " 'incidentally': 23,\n",
       " 'indicate': 70,\n",
       " 'inherent': 32,\n",
       " 'initially': 36,\n",
       " 'interpret': 50,\n",
       " 'interpretation': 102,\n",
       " 'jesus': 261,\n",
       " 'jew': 215,\n",
       " 'jewish': 168,\n",
       " 'judaism': 28,\n",
       " 'lead': 222,\n",
       " 'learns': 2,\n",
       " 'little': 747,\n",
       " 'living': 203,\n",
       " 'man': 368,\n",
       " 'mankind': 36,\n",
       " 'mean': 919,\n",
       " 'metaphor': 11,\n",
       " 'mix': 40,\n",
       " 'modern': 140,\n",
       " 'moral': 147,\n",
       " 'morality': 66,\n",
       " 'moses': 19,\n",
       " 'multiple': 125,\n",
       " 'narrative': 9,\n",
       " 'necessarily': 140,\n",
       " 'never': 849,\n",
       " 'nuance': 4,\n",
       " 'older': 107,\n",
       " 'outside': 222,\n",
       " 'parent': 111,\n",
       " 'patriarch': 4,\n",
       " 'person': 518,\n",
       " 'pharisee': 11,\n",
       " 'piaget': 1,\n",
       " 'pretty': 457,\n",
       " 'pub': 13,\n",
       " 'quite': 543,\n",
       " 'recorded': 40,\n",
       " 'relationship': 93,\n",
       " 'relevation': 1,\n",
       " 'religion': 249,\n",
       " 'required': 208,\n",
       " 'revelation': 53,\n",
       " 'sadducee': 1,\n",
       " 'script': 35,\n",
       " 'secondhand': 13,\n",
       " 'seem': 500,\n",
       " 'shaky': 9,\n",
       " 'simply': 383,\n",
       " 'speculate': 23,\n",
       " 'subjectiveness': 1,\n",
       " 'swear': 20,\n",
       " 'swears': 6,\n",
       " 'talmud': 2,\n",
       " 'told': 357,\n",
       " 'torah': 13,\n",
       " 'trooper': 5,\n",
       " 'trouble': 181,\n",
       " 'type': 411,\n",
       " 'unashamedly': 1,\n",
       " 'understand': 369,\n",
       " 'undoubtably': 4,\n",
       " 'upset': 37,\n",
       " 'utterance': 3,\n",
       " 'water': 135,\n",
       " 'whereas': 53,\n",
       " 'worse': 157,\n",
       " 'wrongness': 2,\n",
       " 'yep': 26,\n",
       " 'yhwh': 1,\n",
       " 'young': 176,\n",
       " '10': 506,\n",
       " '24': 224,\n",
       " '2nd': 119,\n",
       " '3': 817,\n",
       " '3rd': 93,\n",
       " '423': 14,\n",
       " '516': 9,\n",
       " '8': 446,\n",
       " 'achieved': 34,\n",
       " 'acrv': 2,\n",
       " 'activity': 165,\n",
       " 'added': 134,\n",
       " 'adopted': 24,\n",
       " 'advisory': 12,\n",
       " 'air': 166,\n",
       " 'alot': 50,\n",
       " 'also': 1942,\n",
       " 'approach': 164,\n",
       " 'array': 41,\n",
       " 'arrow': 13,\n",
       " 'asis': 5,\n",
       " 'assembly': 49,\n",
       " 'assumes': 41,\n",
       " 'assured': 28,\n",
       " 'b': 283,\n",
       " 'based': 443,\n",
       " 'building': 197,\n",
       " 'buildup': 7,\n",
       " 'bus': 172,\n",
       " 'bus1': 2,\n",
       " 'capability': 126,\n",
       " 'city': 272,\n",
       " 'common': 301,\n",
       " 'communication': 178,\n",
       " 'computed': 16,\n",
       " 'considered': 241,\n",
       " 'control': 461,\n",
       " 'cost': 412,\n",
       " 'crystal': 32,\n",
       " 'currently': 245,\n",
       " 'deleted': 227,\n",
       " 'deployed': 11,\n",
       " 'derived': 47,\n",
       " 'description': 143,\n",
       " 'design': 208,\n",
       " 'developed': 125,\n",
       " 'docked': 3,\n",
       " 'docking': 8,\n",
       " 'dropped': 65,\n",
       " 'edition': 81,\n",
       " 'elvs': 2,\n",
       " 'environment': 136,\n",
       " 'esa': 14,\n",
       " 'eva': 6,\n",
       " 'exception': 73,\n",
       " 'existing': 120,\n",
       " 'external': 125,\n",
       " 'feature': 179,\n",
       " 'finish': 63,\n",
       " 'flight': 86,\n",
       " 'fly': 63,\n",
       " 'force': 328,\n",
       " 'freedom': 140,\n",
       " 'fuel': 71,\n",
       " 'giant': 48,\n",
       " 'gnc': 1,\n",
       " 'griffin': 8,\n",
       " 'habitability': 1,\n",
       " 'habitation': 2,\n",
       " 'helping': 40,\n",
       " 'human': 354,\n",
       " 'inclination': 14,\n",
       " 'initial': 86,\n",
       " 'international': 165,\n",
       " 'jsc': 9,\n",
       " 'keep': 600,\n",
       " 'keeping': 115,\n",
       " 'ken': 75,\n",
       " 'key': 424,\n",
       " 'kw': 3,\n",
       " 'lab': 94,\n",
       " 'language': 174,\n",
       " 'larc': 4,\n",
       " 'launching': 15,\n",
       " 'lerc': 3,\n",
       " 'let': 864,\n",
       " 'lightweight': 7,\n",
       " 'list': 507,\n",
       " 'location': 108,\n",
       " 'lockheed': 8,\n",
       " 'logistics': 4,\n",
       " 'love': 274,\n",
       " 'low': 282,\n",
       " 'major': 315,\n",
       " 'management': 97,\n",
       " 'mantended': 1,\n",
       " 'meeting': 95,\n",
       " 'microgravity': 5,\n",
       " 'mike': 168,\n",
       " 'mission': 95,\n",
       " 'modified': 57,\n",
       " 'modular': 13,\n",
       " 'module': 59,\n",
       " 'msfc': 5,\n",
       " 'nasda': 6,\n",
       " 'node': 15,\n",
       " 'nonsexist': 2,\n",
       " 'obtained': 86,\n",
       " 'occurs': 77,\n",
       " 'oconnor': 6,\n",
       " 'old': 587,\n",
       " 'onboard': 20,\n",
       " 'onorbit': 6,\n",
       " 'onsite': 8,\n",
       " 'ontop': 2,\n",
       " 'opposed': 92,\n",
       " 'optimize': 7,\n",
       " 'option': 227,\n",
       " 'orbit': 88,\n",
       " 'orbiter': 20,\n",
       " 'orbiterspacelab': 1,\n",
       " 'panel': 86,\n",
       " 'partner': 49,\n",
       " 'permanent': 56,\n",
       " 'phase': 42,\n",
       " 'place': 691,\n",
       " 'port': 182,\n",
       " 'power': 595,\n",
       " 'presence': 81,\n",
       " 'presented': 92,\n",
       " 'proposal': 94,\n",
       " 'propulsion': 24,\n",
       " 'provide': 326,\n",
       " 'provides': 125,\n",
       " 'qualified': 33,\n",
       " 'radiator': 12,\n",
       " 'reached': 78,\n",
       " 'redesign': 14,\n",
       " 'removed': 91,\n",
       " 'report': 312,\n",
       " 'reported': 121,\n",
       " 'research': 277,\n",
       " 'respective': 20,\n",
       " 'reston': 7,\n",
       " 'resupply': 3,\n",
       " 'return': 264,\n",
       " 'second': 633,\n",
       " 'sexist': 6,\n",
       " 'shuttle': 72,\n",
       " 'shuttlespacelab': 2,\n",
       " 'six': 133,\n",
       " 'solar': 57,\n",
       " 'sometimes': 246,\n",
       " 'source': 482,\n",
       " 'space': 371,\n",
       " 'srt': 1,\n",
       " 'ssf': 10,\n",
       " 'station': 164,\n",
       " 'sts': 13,\n",
       " 'studied': 36,\n",
       " 'supporting': 68,\n",
       " 'tail': 30,\n",
       " 'tank': 109,\n",
       " 'team': 421,\n",
       " 'tended': 20,\n",
       " 'thermal': 22,\n",
       " 'three': 457,\n",
       " 'today': 425,\n",
       " 'tolerance': 26,\n",
       " 'transferred': 19,\n",
       " 'u': 1223,\n",
       " 'used': 1191,\n",
       " 'utilize': 10,\n",
       " 'vehicle': 133,\n",
       " 'vehilce': 1,\n",
       " 'visit': 72,\n",
       " 'wing': 102,\n",
       " 'wingless': 5,\n",
       " 'yesterday': 91,\n",
       " 'yo': 8,\n",
       " 'york': 167,\n",
       " '1000yds': 2,\n",
       " '12': 381,\n",
       " '1200x': 2,\n",
       " '1400': 24,\n",
       " '2': 1207,\n",
       " '25': 286,\n",
       " '32': 156,\n",
       " '3monolux': 2,\n",
       " '4sunbeam': 2,\n",
       " '5': 639,\n",
       " '50': 341,\n",
       " '525ft': 2,\n",
       " '5band': 2,\n",
       " '5everylast': 2,\n",
       " '6osterizer': 2,\n",
       " '7x35': 2,\n",
       " '80': 175,\n",
       " '8binolux': 2,\n",
       " '9proctor': 2,\n",
       " 'accessory': 27,\n",
       " 'already': 483,\n",
       " 'always': 531,\n",
       " 'amfm': 16,\n",
       " 'angle': 51,\n",
       " 'ask': 459,\n",
       " 'bag': 51,\n",
       " 'behalf': 32,\n",
       " 'bet': 102,\n",
       " 'binoculars': 3,\n",
       " 'black': 238,\n",
       " 'blender': 4,\n",
       " 'brand': 121,\n",
       " 'bro': 2,\n",
       " 'brother': 122,\n",
       " 'cassette': 26,\n",
       " 'contact': 291,\n",
       " 'cookbook': 5,\n",
       " 'deck': 30,\n",
       " 'decker': 2,\n",
       " 'dry': 39,\n",
       " 'dryer': 9,\n",
       " 'dual': 39,\n",
       " 'dubing': 2,\n",
       " 'duster': 2,\n",
       " 'equalizer': 9,\n",
       " 'expeditously': 2,\n",
       " 'extra': 176,\n",
       " 'fixable': 4,\n",
       " 'forsale': 25,\n",
       " 'graphic': 265,\n",
       " 'hair': 44,\n",
       " 'hand': 469,\n",
       " 'head': 306,\n",
       " 'high': 480,\n",
       " 'included': 229,\n",
       " 'includes': 237,\n",
       " 'iron': 31,\n",
       " 'japan': 50,\n",
       " 'lastly': 8,\n",
       " 'leather': 29,\n",
       " 'magnification': 3,\n",
       " 'matic': 2,\n",
       " 'microscope': 4,\n",
       " 'moved': 108,\n",
       " 'moving': 118,\n",
       " 'offer': 358,\n",
       " 'player': 289,\n",
       " 'portable': 60,\n",
       " 'purchased': 70,\n",
       " 'pusle': 2,\n",
       " 'put': 678,\n",
       " 'reasonable': 192,\n",
       " 'reasonablevery': 2,\n",
       " 'reduced': 59,\n",
       " 'reply': 322,\n",
       " 'salon': 2,\n",
       " 'sh': 37,\n",
       " 'silex': 2,\n",
       " 'sound': 442,\n",
       " 'spraysteam': 2,\n",
       " 'sr1000': 2,\n",
       " 'tapetape': 2,\n",
       " 'thru': 49,\n",
       " 'treble': 4,\n",
       " 'underinto': 2,\n",
       " 'vaccum': 2,\n",
       " 'who': 61,\n",
       " 'wide': 118,\n",
       " 'zoom': 23,\n",
       " 'afford': 70,\n",
       " 'camp': 54,\n",
       " 'caused': 134,\n",
       " 'citizen': 198,\n",
       " 'concentration': 26,\n",
       " 'devastation': 3,\n",
       " 'died': 125,\n",
       " 'disease': 108,\n",
       " 'dreamt': 1,\n",
       " 'gassed': 3,\n",
       " 'gassing': 5,\n",
       " 'generally': 184,\n",
       " 'goering': 3,\n",
       " 'happened': 234,\n",
       " 'ii': 171,\n",
       " 'imprison': 2,\n",
       " 'japanese': 58,\n",
       " 'malnutrition': 3,\n",
       " 'nazi': 76,\n",
       " 'originally': 89,\n",
       " 'partly': 19,\n",
       " 'prepared': 72,\n",
       " 'short': 288,\n",
       " 'solution': 219,\n",
       " 'step': 174,\n",
       " 'stick': 119,\n",
       " 'total': 238,\n",
       " 'trial': 100,\n",
       " 'war': 265,\n",
       " 'werent': 76,\n",
       " 'world': 630,\n",
       " 'youre': 521,\n",
       " 'able': 596,\n",
       " 'abuse': 72,\n",
       " 'agrees': 18,\n",
       " 'amount': 235,\n",
       " 'appendix': 11,\n",
       " 'application': 347,\n",
       " 'bizarre': 24,\n",
       " 'capitalized': 7,\n",
       " 'certainly': 367,\n",
       " 'chalk': 3,\n",
       " 'complete': 225,\n",
       " 'complexity': 22,\n",
       " 'deal': 338,\n",
       " 'designed': 188,\n",
       " 'despite': 125,\n",
       " 'effort': 198,\n",
       " 'emphasis': 47,\n",
       " 'every': 704,\n",
       " 'expense': 51,\n",
       " 'flexible': 24,\n",
       " 'format': 179,\n",
       " 'g': 123,\n",
       " 'g1': 3,\n",
       " 'general': 443,\n",
       " 'handle': 160,\n",
       " 'imagegenerating': 1,\n",
       " 'immense': 10,\n",
       " 'import': 37,\n",
       " 'inability': 22,\n",
       " 'interchange': 6,\n",
       " 'job': 275,\n",
       " 'led': 125,\n",
       " 'load': 113,\n",
       " 'making': 374,\n",
       " 'many': 1229,\n",
       " 'mine': 239,\n",
       " 'neither': 171,\n",
       " 'page': 185,\n",
       " 'philosophically': 4,\n",
       " 'poor': 149,\n",
       " 'powerful': 78,\n",
       " 'program': 685,\n",
       " 'reasoning': 67,\n",
       " 'save': 235,\n",
       " 'saying': 379,\n",
       " 'simplicity': 16,\n",
       " 'sort': 386,\n",
       " 'specification': 55,\n",
       " 'success': 92,\n",
       " 'take': 1088,\n",
       " 'theyll': 96,\n",
       " 'tiff': 29,\n",
       " 'trapped': 10,\n",
       " 'unnecessary': 27,\n",
       " 'whenever': 77,\n",
       " 'wont': 407,\n",
       " 'word': 565,\n",
       " 'worried': 48,\n",
       " 'writer': 69,\n",
       " '100': 312,\n",
       " '100000': 34,\n",
       " '1000000': 10,\n",
       " '100k': 7,\n",
       " '11': 217,\n",
       " '1100year': 1,\n",
       " '1150': 5,\n",
       " '1200': 39,\n",
       " '1300': 19,\n",
       " '1300year': 1,\n",
       " '15': 359,\n",
       " '1500': 45,\n",
       " '1500year': 1,\n",
       " '1560': 2,\n",
       " '15x2x': 1,\n",
       " '18': 182,\n",
       " '1986': 54,\n",
       " '1987': 46,\n",
       " '1990': 122,\n",
       " '1992': 164,\n",
       " '2000': 87,\n",
       " '2028': 2,\n",
       " '21': 193,\n",
       " '23': 140,\n",
       " '250': 98,\n",
       " '2500': 24,\n",
       " '26': 128,\n",
       " '27': 115,\n",
       " '28': 113,\n",
       " '2876': 1,\n",
       " '2k': 6,\n",
       " '300000': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary containing the frequency of words utilizing the 'frequency_dict' function\n",
    "\n",
    "# Expect this chunk to take a comparatively longer time to execute since our dataset is large\n",
    "\n",
    "freq_word = frequency_dict(lines)\n",
    "\n",
    "freq_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "vLvPijR_GKHn"
   },
   "outputs": [],
   "source": [
    "# Create a function to calculate the Term Frequency\n",
    "\n",
    "def term_frequency(document, word):\n",
    "    '''\n",
    "    document: list containing the entire corpus\n",
    "    word: word whose term frequency is to be calculated\n",
    "    ---\n",
    "    tf: returns term frequency value\n",
    "    '''\n",
    "    counter = 0\n",
    "    for word_doc in document:\n",
    "        if word == word_doc:\n",
    "            counter = counter + 1\n",
    "    tf = counter / len(document)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "HA99G_yAGLCC"
   },
   "outputs": [],
   "source": [
    "# Create a function to calculate the Inverse Document Frequency\n",
    " \n",
    "def inverse_df(word):\n",
    "    '''\n",
    "    word: word whose inverse document frequency is to be calculated\n",
    "    ---\n",
    "    idf: return inverse document frequency value\n",
    "    '''\n",
    "    count = 1\n",
    "    if word in freq_word and freq_word[word] > 0:\n",
    "        count = freq_word[word]\n",
    "    else:\n",
    "        count = 1\n",
    "    \n",
    "    idf = np.log(total_docs/count)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_cal = {}\n",
    "for word in word_list:\n",
    "    idf_cal[word] = inverse_df(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "F0irgwv2GRfE"
   },
   "outputs": [],
   "source": [
    "#Create a function to combine the term frequencies (TF) and inverse document (IDF) frequencies calculated above to get TF-IDF\n",
    "def tfidf(sentence, dict_idx):\n",
    "    '''\n",
    "    sentence: list containing the entire corpus\n",
    "    dict: dictionary keeping track of index of each word\n",
    "    ---\n",
    "    tf_idf_vec: returns computed tf-idf\n",
    "    '''\n",
    "    tf_idf_vec = [0] * len(dict_idx)\n",
    "    for word in sentence:\n",
    "        if word in dict_idx and tf_idf_vec[dict_idx[word]] == 0:\n",
    "            tfidf = term_frequency(sentence, word) * idf_cal[word]\n",
    "            tf_idf_vec[dict_idx[word]] = tfidf\n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "_VKJhqatGWpV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11314/11314 [00:45<00:00, 250.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#Compute the vectors utilizing the 'tfidf' function created above to obtain a TF-IDF Encoded text corpus\n",
    "vectors = []\n",
    "for line in tqdm(lines):\n",
    "    vec = tfidf(line, dict_idx)\n",
    "    vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE0UGUaSGb8I"
   },
   "source": [
    "### Multinomial Naive Bayes (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "yWYcxrdJGfDC"
   },
   "outputs": [],
   "source": [
    "#Fit a Multinomial Naive Bayes Model on our dataset\n",
    "\n",
    "model = MultinomialNB()\n",
    "model = model.fit(vectors, mydata_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "G6CiQB4qGfqH"
   },
   "outputs": [],
   "source": [
    "#Perform testing on the train dataset    \n",
    "pred = model.predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "yCLagGu6Gh6T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.8837776029534627\n",
      "Accuracy:  0.8880148488598197\n"
     ]
    }
   ],
   "source": [
    "#Calculate the F1 Score and the Accuracy\n",
    "\n",
    "F1_score = metrics.f1_score(mydata_train.target,pred, average='weighted')\n",
    "Accuracy = metrics.accuracy_score(mydata_train.target,pred)\n",
    "print(\"F1 Score: \", F1_score)\n",
    "print(\"Accuracy: \", Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbMRqJv5Gl2F"
   },
   "source": [
    "### Expected Output:\n",
    "F1 Score: 0.9533633964397735\n",
    "\n",
    "Accuracy: 0.9524482941488421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWRDuUqU-taV"
   },
   "source": [
    "Your accuracy does not have to be exactly the same. This is just to give you an estimate of what could you expect your accuracy to be around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfMc8cz93Cc0"
   },
   "source": [
    "## Question 2 Vector Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70iwEeL23F7K"
   },
   "source": [
    "In this unsupervised learning task we are going to cluster wikipedia articles into groups using T-SNE visualization after vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHx4YuxW36oM"
   },
   "source": [
    "### Collect articles from Wikipedia (10 points)\n",
    "\n",
    "In this section we will download articles from wikipedia and then vectorize them in the next step. You can select somewhat related topics or fetch the articles randomly. \n",
    "(Use dir() and help() functions or refer wikipedia documentation)\n",
    "You may also pick any other data source of your choice instead of wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jA419x6__mjg"
   },
   "outputs": [],
   "source": [
    "#install libraries\n",
    "#pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "vLMLk4K84Zbn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in:Machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shadow/opt/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/shadow/opt/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in:Data\n",
      "Error in:Bank of America\n",
      "Error in:Bank\n",
      "Error in:Visa Inc.\n",
      "Error in:Mastercard\n",
      "Error in:Bank\n",
      "Error in:Bank of the United States\n",
      "Error in:Bank of America\n",
      "Error in:Swimming (sport)\n",
      "Error in:Swimming pool (disambiguation)\n",
      "Error in:Tennis\n",
      "Error in:Rafael Nadal\n",
      "Error in:2022 College Football Playoff National Championship\n",
      "Error in:FIFA\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "\n",
    "'''\n",
    " Generate a list of wikipedia article to cluster \n",
    " You can maintain a static list of titles or generate them randomly using wikipedia library\n",
    " Some topics include:\n",
    " [\"Northeastern Unversity\", \"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \"Data science\", \"Master in Data Science\", \n",
    " \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Bank\", \"Financial technology\",\"International Monetary Fund\", \n",
    " \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"College Football\", \"Association Football\"]\n",
    "\n",
    " You can add more topics from different categories so that we have a diverse dataset to work with. \n",
    " Ex- About 3+ categories(groups), 3+ topics in each category, 3+ articles in each topic\n",
    "'''\n",
    "\n",
    "# selected topics\n",
    "topics = [\"Natural language processing\", \"Machine learning\", \"Quantum machine learning\", \"Artificial intelligence\", \"Data science\", \"Master in Data Science\", \n",
    " \"Bank of America\", \"Visa Inc.\", \"European Central Bank\", \"Bank\", \"Financial technology\",\"International Monetary Fund\", \n",
    " \"Basketball\", \"Swimming\", \"Tennis\", \"Football\", \"College Football\", \"Association Football\"]\n",
    "# list of articles to be downloaded\n",
    "articles = []\n",
    "for topic in topics:\n",
    "    sub_top = wikipedia.search(topic)\n",
    "    articles = articles + sub_top[:10]\n",
    "\n",
    "# download and store articles (summaries) in this variable\n",
    "data = []\n",
    "for article in articles:\n",
    "    try:\n",
    "        data.append(wikipedia.summary(article))\n",
    "    except WikipediaException:\n",
    "        print(\"Error in:\" + article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgpRv7wQ4Dpm"
   },
   "source": [
    "### Cleaning the Data (5 points)\n",
    "In this step you will decide whether to clean the data or not. If you choose to clean, you may utilize the clean function from assignment 1.\n",
    "\n",
    "**Question:** Why are you (not) choosing to clean the data? Think in terms of whether cleaning data will help in the clustering or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnZpDKcaHTGq"
   },
   "source": [
    "**Answer(1-3 sentences):** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "lNj53Pxr963N"
   },
   "outputs": [],
   "source": [
    "# You can use Assignment 1's clean message function\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "clean_data = []\n",
    "for line in data:\n",
    "    clean_data.append(clean_text(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvRZUpmq-DmT"
   },
   "source": [
    "### Vectorize the articles (5 points)\n",
    "\n",
    "In this step, we will vectorize the text data. You can use TfidfVectorizer() or countVectorizer() from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "gJk8YY89-OU4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "DAIGlqEuINWA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 4703)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKLvrKHRQaQq"
   },
   "source": [
    "### Sample Output:\n",
    "(36, 1552)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5ZrGrzD_G8d"
   },
   "source": [
    "### Plot Articles (10 points)\n",
    "Now we will try to verify the groups of articles using T-SNE from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "SjcuZBOe-oZq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 2)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from sklearn.manifold import TSNE\n",
    "\n",
    "# call TSNE() to fit the data\n",
    "X_embedded = TSNE(n_components=2, init='random', learning_rate='auto').fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCY_blxjO1bs"
   },
   "source": [
    "Plot and annotate the points with different markers for different expected groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "3ODUA1Vf-rRd",
    "outputId": "325b5db4-60d1-4907-a487-40893e256ee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fdcc9482e80>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAHSCAYAAAAwpbX/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnoElEQVR4nO3dX4yc13kf4PfsEtuIAYkmsGxtuOHSCpwiluEq8MJAEbVIkH9OUJBxgAAOFoWhRmBkxve1QCwUNSAUpElz0UquGMiOAa1j+Ma10aRRbF/UgNAgoVAnlZK4ERWSoUTYTnMhFYzMSDy9+HbF2d2Z2Z2d7//3PMBiON9QO8ez450f33POe1LOOQAAKM9C0wMAAOgbAQsAoGQCFgBAyQQsAICSCVgAACUTsAAASnak6QGMesc73pFPnTrV9DAAAPb1/PPP/13O+e5xj7UqYJ06dSouXbrU9DAAAPaVUro66TFThAAAJROwAABKJmABAJRMwAIAKJmABQBQMgELAKBkAhYAQMkELACAkglYAAAlE7AAAEomYAEAlEzAAgAomYAFAFAyAQsAoGQCFgBAyQQsAHphZSUipb1fKytNj4whErAA6IXTpyOWlnZeW1qKOHOmmfEwbAIWAL2wsRGxsOtTbXGxuA51E7AA6IXl5YgHH7xTxVpaKu7fc0+z42KYBCwAemO0iqV6RZMELAB6Y7uKtbCgekWzBCwAemVjI+LUKdUrmnWk6QEAQJmWlyMuX256FAydChYAQMkELACAkglYAAAlE7AAAEomYAEAlEzAAgAomYAFAFAyAQsAoGQCFgATraxEpLT3a2Wl6ZFBuwlYQO8JCYd3+nTE0tLOa0tLEWfONDMe6AoBC6hUG8KNkHB4GxvFwcmjFhed8wf7EbCASrUh3AgJh7e8HPHgg3d+hktLxf177ml2XIfVhsDPMAhYQKXaEG76FhLqNvoz7HowbUPgZxgELKBSbQk3fQoJddv+GS4sdD+YtiHwMwylBKyU0qdSSt9KKb0wcu17U0pfTin99dbt95TxXED3tCHc9CkkNGFjI+LUqe4HkbYEfvqvrArW70bEh3Zd+0REfDXn/J6I+OrWfWCA2hJu+hISmrC8HHH5cj+CSBsCP/1XSsDKOX8tIv5+1+UzEfGZrT9/JiJ+roznArqpDeGmTyGBw2tL4KffqlyD9a6c842IiK3bd1b4XEDLCTe0SRsCP/12pOkBpJTORsTZiIiTJ082PBoAhmA78ENVqqxgfTOltBwRsXX7rXF/Ked8Mee8lnNeu/vuuyscDgBAPaoMWF+KiI9u/fmjEfHFCp8LAKA1ymrT8HsR8T8j4p+llK6nlH4pIn49In4ypfTXEfGTW/cB6AEd0WG6UtZg5Zx/ccJDP17G9wegXU6fjnj66Yhbt+5c0xEd7tDJHYCZ6YgO0wlYAMxMR3SYTsAC4FB0RIfJBCygtSykbjcd0WEyAQtordOn70xBbbOQul10RIfxBCygtSykbj9HIMF4AhZ7bW4W/yRdWChuNzebHhEDZSE10FUCFjttbkacPRtx9WpEzsXt2bNCFo2xkJomWP/HvAQsdjp/PuLmzZ3Xbt4srsMYVX8QWUhNE6z/Y14CFjtduzbbdQavjg8iC6mpm/V/zEvAYqeTJ2e7zuDN+kF0mIpXWQupTfuUq8+vp/V/zEvAYqcLFyKOHt157ejR4jqMMesHUZNTL6Z9ytX317Oq9X99DqbcIWCx0/p6xMWLEaurxf/jV1eL++vrTY+MFpvlg6jJqRfTPuXq++tZ1fq/vgdTCgIWe62vR1y5EnH7dnErXLGPWT6Impx6Me1TriG8nuPW/81bgep7MKUgYFGosveVvlqDMMtC9CZbL8z63KZzput7G41x6//mrUANIZgiYBFRbe8rfbUGY5aF6E22Xpj1uU3nTDfENhplVKD6HkyJiJxza74+8IEPZA7hmWdyXl3NOaXi9plnZvvvV1dzLuLPzq/V1fnHVuX3ptNefTXne+/N+caNdj/3q6/m/F3ftfPte9ddzYy7rZr8WZblxInxv6pOnBj/9z/2sZyXloq/s7SU87lzsz/nxz6W88LC4f5b2iEiLuUJmSYVj7fD2tpavnTpUtPD6JbtCtFoc9CjR2dbmL6wUPwu2S2lYh3WPKr83lCTc+cinn464tatonr10EMRTzzR9Kgo0+jPeNu0n/WNGxH33hvxxhsRd90V8fLLs1fvbtyIeOCBiOeeG0blr49SSs/nnNfGPWaKsOvK6LxeZe8rfbXoAdM5/TfrtF8ZU6MOyu43Aavryui8XmXvK3216IEhrjMamsMsPHfCANMIWF1XRoWoyt5X+mrREz5M+2/WSqUKFNNYg9V1ZazBAiAiirVYTz0V8fDD1tmxP2uw+kyFCKA0KpWURQULAOAQVLAAAGp0pOkBAExy/PHj8fqt1/dcP7Z0LF575LUGRgRwMCpYQGuNC1fTrgO0hYAFAFAyAQsAoGQCFgBAyQQsoFQrK0VLtt1fKytNjwygPgIWUKrTp++c57ZtaSnizJnZv9expWMzXQdoC41G+25zM+L8+eLw55Mni0OWdXmnQjduRNx7b8Qbb9y5dtddES+/7My2IVtZiXjllb3XT5yIuH69/vFAGTQaHartcwqvXo3Iubg9e7a4DhVZXo548ME7VaylpeK+cNV980z/llnZhC4QsPrs/Pmdh0BHFPfPn29mPAzGxkbEwtZvl8VF57r1xTwhafQ9sc17gz4TsNpuc7M4eXRhobidpfp07dps16Ek21WshQXVqz6ZJySpbDI0AlabzTvFd/LkbNehRBsbxb8JVCj6Y96QpLLJkAhYbTbvFN+FCxFHj+68dvRocR0qtrwccfmyCkXfzBOSVDYZEgGrzead4ltfj7h4MWJ1tViJurpa3LeLEHqvqn5k84YklU2GQsBqszKm+NbXI65cibh9u7gVrmAQ9luQPk8AmyckqWwyFAJWm5niAw5pvwXp8+wIFJJgfwJWm7Vxim+eXY1AbfZbkK5tAlRLJ3cObntX4+jC+6NHmw99wFijXfXHddM/dy7i6acjbt0qAthDD0U88URz44Wu0cmdcmhcCp2y34J0bROgOgIWB6dxKS1Q1e64vpq2IF3bBKiOgMXBaVxKCzjTbjb7LUjXNgGqIWBxcHY10gIWZ5fLjkCohoDFwbVxVyOD40w7oAvsIgQ6Z7/dcQB1sIsQ6BWLs4G2O9L0AAAOY2Mj4tlnrb1ispWViFde2Xv9xImI69frH0/bHX/8eLx+6/U9148tHYvXHnmtgRF1mwoW0EkWZ7MfO05nMy5cTbvOdAIWwAR6bnWbHac0ScDqO2cHNsoHdLepgHSbHac0ScBqQl2hZ/vswKtXI3Iubs+eFbJq5AO621RAus9xQDRFwKpbnaHH2YGN8wHdbSog3WfHKU0RsOpWZ+hxduD85qw2+oDuPhWQ7nMc0MEcWzo203Wm02i0bgsLReVqt5Qibt8u97lOnSoqZLutrkZcuVLuc/XRdrVxNBAfPTpz93pNMbvv3LmIp56KePjhiCeeaHo0QFtoNNomdR6Y7OzA+ZRUbTRF0X19qoDYeEHVjj9+PNJjac/X8cePNz20WglYdasz9Dg7cD4lTrH26QN6iPrUc8vGC6qmn1bBFGETNjeLKsi1a0Xl6sIFoaeNTLHSQ6NT1ttMXVOm9Fia+Fh+tD2ZowymCNtmfb34gL59u7gVrtrJFCs9ZOMF1EPA6iLNQ+thipWeGtLOSGvOaIqA1TWah9ZLtZEeasvGizrCjzVnNEXA6hrNQ4EStGHjRR3hR7Pf+umnVbDIvWvq7KMFUKG6FtyfOxfx9NMRt24VAe6hh/QzoxwWufdJnX20ACpU14L7Ia05oz0ErK6xsw3okTrCT1vWnHFwfWhWKmB1jZ1tBTspoRfqCj+HWXNmB2Jz+tCsVMDqoqHvbLOTEnqljgX3h+nGbwci86h8kXtK6UpEvB4Rb0XEm5MWg0VY5M4B6bAO1GCoXe+PP358bKXo2NKxeO2R12oZQ1e6wbdhkfuP5Zzvnxau4MBKPCMQYJKhdr3vw/RcG5gipHvspARqYgcih1VHwMoR8UcppedTSmdreD76zk5KoCZ2IDajD81Kj9TwHD+Sc341pfTOiPhySumvcs5f235wK3SdjYg4qQLBQWwv6j9/vpgWPHmyCFdDW+wP1GJjI+LZZ1Wv6lTXWq8q1drJPaX0qxHx/3LOvznucYvcAaBZXVlg3gaNLXJPKX13SunY9p8j4qci4oUqnxMAOLw+TM+1QdVThO+KiC+klLaf67M55z+s+DkBGJCVlYhXXtl7/cSJiOvX6x9P1/Vheq4NKg1YOeeXI+KfV/kcAAzb6dN3DnPepiEoTdOmARy7A5022kphm5YK7deH8wanEbDoh8OGJMfuQOcNtSFo1/W9oWmtuwj3Yxchh7Idkm7evHPt6NGDHYLt2B3ohdFjbYZwnM2oNhxtcxh92K04bRdhHX2woFrnz+8MVxHF/fPn9w9Yjt2BXtiuYj311PCqV1VWgroa3trAFCHdN09IcuwO9MbGRlGUtvaqPH2fxquSgEX3zROSHLsDvbG8HHH58rCqV7SXgEX3zROS1teLtVqrqxEpFbcHWbsFW1ZWirfO7q+VlaZHBu3W94am1mDRffOeTbi+vv/f3dx09iFj6cEEh9P3NVwqWPTD+nqx6+/27eK2zPCjlQNT6MFE0/peCeoqFSzYzzy7FOm97d1r21UsPZioW5WVoGNLxybuImQ6fbBgPwsLReVqt5SKihmDN+QeTDBk0/pgmSKE/VTYysEC6W7b/vl93/cV4Soi4h/+IWJt7K9bYEgELNhPha0cTp++c7zHNguku8PPD5hEwIL9VNjKwQLpbvPzAyYRsOAgKtql6JDaZpQ1NevnB0xikTs0zALp+p07N7531UMPRTzxxGzfy8+PrnLO4Pwscqcem5vFQWALC8WtPlEHsl0FWVgor/ph8fx0ZU7tVfHzgzo4Z7BaAhbl0IxzLmUfUmvx9XRlT+315ZDhxcXxwXxxsemRQfeYIqQcp04VoWq31dVizRK1Gp222mb6aidTe3u9730RL7649/p990W88EL946Fa6bE08bH8aHuyQZuZIqR6167Ndp1KWXy9P1N7ez3zzPjrn/1sveOAPhCwKEeFzTg5nNF1RloHjNeXqb2y3H9/Ua0add99Ee9/fyPDgU4TsChHhc04ORwVmv0tL0dcvuy1GbW7iqV61V8Oia6WNViUZ3OzOAD52rWicnXhQvOHIbdxTDW6cSPigQcinntOiKjTykrEK6/svX7iRMT16/WPZ1bba7GsvYLprMGiHhU14zw0OxtVaBrS9V2czzwTceSI6hXMQwWL/rKzsVRdr8rUyS5OGAYVLIbJzsZSdb0qUye7OAEVLPpLBatUqjKz0WeLtnE0TvlUsBgmOxtLpSozG7s4aRtH49RLwKK/1tcjLl4sKlYpFbcXLza/+L7D9NaajT5bMFwCFv3Wtp2NHacqMxu7OGG4BCxgJqoyAPs70vQAgG7ZrsoAMJkKFgAMgKNx6qWCBQADoBVDvQQsACbSwZ+D0mdrJ1OEAEykgz8Hpc/WTgIW8LaVlaJl2O6vlZWmR0ZTRnufbdMDDfY32IDlgwT2Uq1gNx384XAGG7B8kMBeqhWMo4M/zG6wAcsHCfs6dy7iyJGitHnkSHG/51QrGEcHf5jdYAOWDxKmOncu4pOfjHjrreL+W28V9wcQslQrGEcHf/ajz9ZOKefc9Bjetra2li9dulTb8924EXHvvRFvvBFx110RL78sYLHlyJE74WrU4mLEm2/WP56anTsX8dRTEQ8/HPHEE02PBqCdUkrP55zXxj022ApWhLI3U4wLV9Ou98ws1QobRgD2GnTAilD2ZoLFxdmu98z2eYMH+UeHDSMAew0+YM3yQcKAnD072/UBs2EEYK/BBywY68knIz72sTsVq8XF4v6TTzY7rllsbhbl2YWF4nZzs5KnsWEEYC8Bi36oIkw8+WSxoD3n4rZr4ers2YirV4vxX71a3K8oZNl5CLCTgEX31RwmOuH8+YibN3deu3mzuF4BG0YAdhp0mwZ64tSpIlTttroaceVK3aNph4WFImzullLE7dtv311ZiXjllb1/7cSJiOvXZ3vKGzciHngg4rnnBCxgGLRpYHY1rd8pxbVrs10fgpMnD3S9zB2ANowA3CFgsVfXptwOGCYG5cKFiKNHd147erS4PsIOQIBqCFjsVfP6nbkdMEwMyvp6xMWLxTRpSsXtxYvF9RF2AAJUwxos9jrg+p1W2dwsAuC1a0Xl6sKFPWGC8RwZRR8tLo7/dbWwMJgDGaiBNVjMpotTbuvrxYL227eLW+HqwOwApI9+6Idmuw5lE7DYy5Tb4Dgyir555pnx1z/72XrHwXAJWOx1wPU79IcdgPTN/fdH3Hffzmv33Rfx/vc3MhwGyBosAHrp61+P+OEfvnP/z/5MwKJc1mABMDijVSzVK+omYAHQW888E3HkiLVX1E/AKluXOqAD9Nz990f84z+qXlG/I00PoFe2O6BvN+nc7oAeYYE4AAyIClaZutYBHQCohIBVJocOAwAhYJWrix3QAYDSCVhl0gEdAAgBq1w6oAMAIWAVymyt4NBhABg8bRq0VgAASqaCpbUCAFAyAUtrBQCgZJUHrJTSh1JK30gpvZRS+kTVzzezrrdWcDQPALROpQErpbQYEU9ExM9ExHsj4hdTSu+t8jln1uXWCtvrx65ejcj5zvoxIQsAGlV1BeuDEfFSzvnlnPOtiPhcRJyp+Dln0+XWCtaPAUArVR2wTkTE347cv751rV262lqh7PVjphsBoBRVB6w05lre8RdSOptSupRSuvTtb3+74uH0TJnrx0w3AkBpqg5Y1yPi+0fur0TEq6N/Ied8Mee8lnNeu/vuuyseTs+UuX7MdCMAlKbqgPWnEfGelNK7U0pLEfGRiPhSxc85HGWuH9OuohErK8WPbvfXykrTIwNgHpUGrJzzmxHx8Yh4NiL+MiI+n3N+scrnbL2y1zmVtX6s6+0qOur06YilpZ3XlpYizrRrKwgAM6q8D1bO+Q9yzj+Yc/6BnHMHeh9UqM3rnLrcrqLDNjaKrD1qcbG4DkB36eRepzavc+pyu4oOW16OePDBO1WspaXi/j33NDsuAOYjYB3WYab62r7OqavtKjputIqlegXQDwLWYRx2qs86J8bYrmItLKheNcmGA6BMAtZhHHaqzzonJtjYKAqhqlfNseEAKFPKOe//t2qytraWL1261PQw9rewUFSudkupmF6bZnOzCGLXrhWVqwsXTMVBC9y4EXHvvRFvvHHn2l13Rbz8sqoiMF5K6fmc89q4x1SwDmOeqT7rnKCVbDgAyiRgHYapPuglGw6AsghYh6GlAfSSDQdAWazBAhhx40bEAw9EPPecgAVMN20N1pG6BwPQZsvLEZcvNz0KoOtMEQIAlEzAAgAomYA1zWGOwwEABs8arEm2j8PZ7ti+fRxOhN2CAMBUKliTHPY4HABg8ASsSa5dm+06AMAWAWuSeY7DAQAGTcCaxHE4AMAhCViTOA4HADgkAWua9fWIK1cibt8ubusOV+fORRw5UgS8I0eK+wBA62nT0FbnzkV88pN37r/11p37Tz7ZzJgAgANRwWqrixdnuw4AtIaA1VZvvTXbdQCgNUwRttXi4vgwtbhY/1iAQTr++PF4/dbre64fWzoWrz3yWgMjgu5QwWqr7WN5DnodoGTjwtW068AdKlhttb2Q/eLFopK1uFiEKwvcAaD1BKyqbW4W5xdeu1Z0gb9w4eDtHp58UqCCGpgKY7eVlYhXXtl7/cSJiOvX6x8P3WOKsEqbm0XV6erViJyL27Nni+t9trkZcepUxMJCcdv3/710nqkwdjt9OmJpaee1paWIM2eaGQ/dI2BV6fz5iJs3d167ebO43ldDDZVAr2xsFP9GHLW4WFyHgzBFWKVr12a73gfTQqVjhqBTji0dmzh12nfLyxEPPhjx9NMRt24V1asHH4y45556x2H6ursErCqdPFlUcMZd76shhspp5lmDBw0b+gf4xkbEpz9d/Lmp6pXp6+4yRVilCxcijh7dee3o0eJ6X00Kj30OlZOYLoVO265iLSw0U72i24YVsOpefL2+XrRZWF0tDmxeXS3u97mCMcRQOckQ1+B11KQpryFMhTHdxkbxcWHtFbNKOeemx/C2tbW1fOnSpWq++XY1YfQD7+jR/geeJpgWKywsFJWr3VKKuH27/vEAnZMeSxMfy4+25/N7qFJKz+ec18Y9NpwKlmpCfdbXI65cKULElSvDDFcRpksBBmw4Acvia+pmuhSYk+nr7hrOLsIh7uijWduVO9OlwCENfSdnlw2ngqWaQBNMlwIM0nAC1hB39A2dI3sAaMhwAlaEasJB9CWU6EEFQIOGFbCYrk+hxK5RABo0nD5Y7O/UqfEbAVZXi4pfl+hBBZVyRh7og8VB9amVhR5UUCln5MF0w2nTwP761MriwoXxnfvtGoWxVKSgXCpY3FFnK4uqF9PbNQozUZGCcqlgcUddjTF3nwu5vZh+dAxlWF8XqABohIDFTnWEkmk7/AQioMNMtbLNFCH169NiehgoZ+SNN22qNT2W4vjjx2seEU1RwaJ+fVpMv5/NzfKnXKv4njCjKqsxfa4CWdM2HAIW9RvKDr8q1prVtX6tR/r8YV2mY0vHJr5OdbPgnj4wRdhHbT/uZig7/KroJq9D/cx8WB/Ma4+8FvnRvOdLCIXDEbD6puzjbqoKa0M4F7KKtWbWrwF0goDVN2VWOPp0NmETqugmr0M9tNrQF/lzh4DVN2VWOPowHdXkdGkVjVvrbAYLzGx7qtUuSyxy75syd+h1fTqq6QXhVTRurasZLDSoTQvuD8vaNVLOuekxvG1tbS1funSp6WF02+5QEVFUOA6ziPzUqfFhbXW1WDfVdl0fP6WwixCoSkrp+Zzz2rjHVLD6pswKR9fbKXS9AkcpZg1RAhlQBmuw+qisHXpdb6dgQTiHoK0DUAYBi+m63E7BgnAAGmKKkP6yIBxoKVPR/Sdg0W/r6wIVtfPhyX5MRfefKUKAkvnwBAQsgBEaRAJlMEUIZdjctNarJ0zhAWUQsGBeTXeMBwbNmr92MkUI8+rDmY1Arcqcirbmr51UsIbA9FW1dIxnlz6cpUe1VJb6T8DqO9NX1SvzgG16wYcnYIqw70xfVU/HeAB2EbD6zvRV9bp+ZiMApassYKWUfjWl9EpK6etbXz9b1XMxhQOP57e5GXHqVMTCQnG7ubn373T5zEag0/Rua6eq12D9ds75Nyt+Dqa5cGHnGqwI01ezKGsNm40GsIf2AuXwWrWTRe5958Dj+Uxbw3bQ19BGA1ooPZYmPpYfzbWMYd72Am343wCTVL0G6+MppT9PKX0qpfQ9FT8Xk5i+Orwy1rDZaAAwOHMFrJTSV1JKL4z5OhMRn4yIH4iI+yPiRkT81oTvcTaldCmldOnb3/72PMOB8pWxhs1GA4DBmStg5Zx/Iuf8vjFfX8w5fzPn/FbO+XZE/E5EfHDC97iYc17LOa/dfffd8wwHyldGCwYbDQAGp8pdhMsjdz8cES9U9VxQmTJaMOiTBTA4VS5y/42U0v0RkSPiSkT8coXPBdVZX59v3ZqNBjCWI4Xos8oCVs7531T1vWkx7QjGmzekQQ/1vb2ANhTDpk0D5dGOADqjD20M2v6/Yd42FHSbo3Ioj3YEABARAhZl0o4AACJCwKJM2hEAQEQIWJRJOwIAiAiL3CmTdgR0hN1d1EEbimETsCiXdgR0gN1d1KGqsO4fCN1gihAAOsQ/ELpBwAIAKJmABQBQMgELAKBkAhbstrkZcepUxMJCcbu52fSIKNmkXVx2dwFlsYsQRjV5nqKDsg9t1l1Vdlp1lx102j90hYAFo6adp1hl2HFQ9lzsqqpfU0HHz9o/ELrCFCGMauo8RQdl0zGCDkynggWjTp4sqkfjrlfJQdm1ML0E1EXAglEXLuycqouo5zzFpoLdwPS96iJAQnuYIoRR6+sRFy9GrK5GpFTcXrxY/TooB2VTgr4HSOgSAQt2W1+PuHIl4vbt4raOReZNBbue0HZhOPys6QpThNAWDso+NNNf9WuqVYCfNV0hYAEws74HnUnr2SKsaeNgTBECg2F6iYOatm7NmjYOQgULGIy+Vx10+Ib2ELAAeqLvARK6xBQhAEDJBCwAgJIJWACwy7R1a9a0cRDWYAHALtazMS8BCwBq5MzIYRCwACiVADGdMyOHQcAC2CIYlEOAAIvcAd4mGABlEbAAAEomYAEAlEzAAoAaOXR8GCxyB+iRNizUd+j0dDZMDIOABbClD8GgDQv1BQgQsADeJhgAZRGwAGit9Fia+Fh+NNc4EpiNRe4AACUTsBiMlZWIlPZ+raw0PTIA+kbAYjBOn45YWtp5bWkp4syZZsYDVdACANrBGiwGY2Mj4tOf3nltcbG4Dn1hoT60gwoWg7G8HPHgg3eqWEtLxf177ml2XAD0j4DFoGxsRCxsvetVrwCoiilCBmW7ivXUU6pX0AVaMdBVKlgMzsZGxKlTqlcAVEcFi8FZXo64fLnpUQDQZwIWANAZbTjQ/CBMEQIAndGGA80PQsACACiZgAUAUDIBCwCgZBa5AwCtMm0he1eoYAEArTJtIXtXDjRXwQIAOqNNrRimEbAA2FdXeg9BW5giBGBfXek9BG0hYAEAlEzAAgBapSsL2aexBgsAaJU+rOtTwQIAKJmABcC++jBlA3UyRQjAvvowZQN1UsECACiZgAUAUDJThABAL7TpxAEVLACgF9p04oCABQBQMgELAKBkcwWslNIvpJReTCndTimt7XrskZTSSymlb6SUfnq+YQIAdMe8i9xfiIifj4inRi+mlN4bER+JiPsi4vsi4isppR/MOb815/MBAC3WpoXmTZqrgpVz/suc8zfGPHQmIj6Xc/5OzvlvIuKliPjgPM8FALRfkwvN23TiQFVtGk5ExB+P3L++dQ0AoBJtqpDtG7BSSl+JiHvGPHQ+5/zFSf/ZmGt5wvc/GxFnIyJOnjy533AAmMDUDF3Xp/fwvgEr5/wTh/i+1yPi+0fur0TEqxO+/8WIuBgRsba2NjaEAbC/NvUAgsPo03u4qinCL0XEZ1NK/zGKRe7viYg/qei5AGBmfaqW1MVrdnDztmn4cErpekT8i4j4/ZTSsxEROecXI+LzEfEXEfGHEfErdhAC0CZ9qpbU5SCvWZsWmjdprgpWzvkLEfGFCY9diIgL83x/AKBbVLIKOrkDAJRMwALoCVMzdF2f3sNVLXIHoGamZrrDYvHx+vS/XQULgEFqslrS1QX2faowVU0FC4BB6lO1pC5es4MTsABgCtN5HIYpQgCYoqvTeTRLBQsAaKUuVw9VsACgZhaLH0yXq4cqWABQs7ZXX5ifChYAQMkELACYwnQeh2GKEACmMJ3HYahgAQCt1OXqoQoWQMPSY2niY/nRXONIoF26XD1UwQIAKJmABQBQMgELAKBk1mABULkuH3lC+7Xx/aWCBUDlunzkCe3XxveXgAUAUDJThAAN04oB+kcFCwCgZAIWAEDJBCwAKtflI09ovza+v6zBAqByWjFQpTa+v1SwAABKpoIFAHOa1OhyHM1Vh0EFCwDmNEtDS81Vh0HAAgAomYAFAFAya7AAYEQbDw6me1SwAGBEGw8OpnsELACY0ywNLTVXHQZThAAwJ1OH7KaCBQBQMgELAKBkAhYAjGjjwcF0jzVYADDCeirKoIIFAFAyAQsAoGQCFgBAyQQsAICSCVgAACUTsAAASiZgAQCUTMACACiZgAUAUDKd3AEYjOOPH4/Xb72+5/qxpWM6uFMqFSwABmNcuJp2HQ5LwAIAKJmABQBQMgELAKBkAhYAQMkELAAG49jSsZmuw2Fp0wDAYGjFQF1UsAAASiZgAQCUTMACACiZgAUAUDIBCwCgZAIWAEDJBCwAgJIJWAAAJROwAABKJmABAJRMwAIAKJmABQBQMgELAKBkAhYAQMkELACAkqWcc9NjeFtK6dsRcbWmp3tHRPxdTc/VJV6X8bwu43ldxvO6jOd12ctrMl5XXpfVnPPd4x5oVcCqU0rpUs55relxtI3XZTyvy3hel/G8LuN5XfbymozXh9fFFCEAQMkELACAkg05YF1segAt5XUZz+syntdlPK/LeF6Xvbwm43X+dRnsGiwAgKoMuYIFAFCJwQWslNJ/SCn9VUrpz1NKX0gp/dORxx5JKb2UUvpGSumnGxxm7VJKv5BSejGldDultDZy/VRK6R9SSl/f+vovTY6zbpNel63HBvt+GZVS+tWU0isj75GfbXpMTUkpfWjr/fBSSukTTY+nLVJKV1JK/3vr/XGp6fE0JaX0qZTSt1JKL4xc+96U0pdTSn+9dfs9TY6xCRNel87/XhlcwIqIL0fE+3LO74+I/xMRj0REpJTeGxEfiYj7IuJDEfFkSmmxsVHW74WI+PmI+NqYxy7nnO/f+nq45nE1bezr4v2yx2+PvEf+oOnBNGHr5/9ERPxMRLw3In5x631C4ce23h+d3no/p9+N4vfFqE9ExFdzzu+JiK9u3R+a3429r0tEx3+vDC5g5Zz/KOf85tbdP46Ila0/n4mIz+Wcv5Nz/puIeCkiPtjEGJuQc/7LnPM3mh5H20x5XQb9fmGsD0bESznnl3POtyLic1G8TyAiInLOX4uIv991+UxEfGbrz5+JiJ+rc0xtMOF16bzBBaxd/m1E/PetP5+IiL8deez61jUi3p1S+l8ppf+RUvqXTQ+mJbxfdvr41rT7p4Y4xbHFe2KyHBF/lFJ6PqV0tunBtMy7cs43IiK2bt/Z8HjapNO/V440PYAqpJS+EhH3jHnofM75i1t/53xEvBkRm9v/2Zi/36stlgd5Xca4EREnc87/N6X0gYj4ryml+3LOr1U20Jod8nXp/ftl1LTXKCI+GRG/FsX//l+LiN+K4h8vQzOo98SMfiTn/GpK6Z0R8eWU0l9tVS1gks7/XullwMo5/8S0x1NKH42Ifx0RP57v9Km4HhHfP/LXViLi1WpG2Iz9XpcJ/813IuI7W39+PqV0OSJ+MCJ6s1D1MK9LDOD9Muqgr1FK6Xci4r9VPJy2GtR7YhY551e3br+VUvpCFNOpAlbhmyml5ZzzjZTSckR8q+kBtUHO+Zvbf+7q75XBTRGmlD4UEf8uIk7nnG+OPPSliPhISumfpJTeHRHviYg/aWKMbZJSunt78XZK6d4oXpeXmx1VK3i/bNn6UNj24Sg2BgzRn0bEe1JK704pLUWxCeJLDY+pcSml704pHdv+c0T8VAz3PTLOlyLio1t//mhETKqaD0offq/0soK1j/8cEf8kijJ1RMQf55wfzjm/mFL6fET8RRRTh7+Sc36rwXHWKqX04Yj4TxFxd0T8fkrp6znnn46IfxUR/z6l9GZEvBURD+ece7cYcZJJr8vQ3y+7/EZK6f4oSvlXIuKXGx1NQ3LOb6aUPh4Rz0bEYkR8Kuf8YsPDaoN3RcQXtn7fHomIz+ac/7DZITUjpfR7EfGjEfGOlNL1iHg0In49Ij6fUvqliLgWEb/Q3AibMeF1+dGu/17RyR0AoGSDmyIEAKiagAUAUDIBCwCgZAIWAEDJBCwAgJIJWAAAJROwAABKJmABAJTs/wMmPMRYYekh0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a figure handle\n",
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "ax.scatter(X_embedded[0:50,0],X_embedded[0:50,1], marker = 'o', c = 'r')\n",
    "ax.scatter(X_embedded[50:100,0],X_embedded[50:100,1], marker = 'v', c = 'b')\n",
    "ax.scatter(X_embedded[100:165,0],X_embedded[100:165,1], marker = 's', c = 'g')\n",
    "\n",
    "# number the points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYcEi1EC_UGO"
   },
   "source": [
    "**Question:** Comment about the categorizion done by T-SNE. Do the articles of related topics cluster together? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tde1Wu5HI6AA"
   },
   "source": [
    "**Answer(1-3 sentences):**  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2xrJddIpSsd"
   },
   "source": [
    "# Question 3 Building Neural Networks\n",
    "\n",
    "### We are gonna use Emotions Dataset for this task. We need to classify the given text into different kind of emotions like happy,sad,anger etc.., \n",
    "\n",
    "### We are providing train.txt and val.txt files along with this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJYmVbkvphgX"
   },
   "source": [
    "### Library Imports and Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0D5wVjZw7s0"
   },
   "source": [
    "### Q) Importing the datasets and do the necessary cleaning and convert the text into the vectors which are mentioned in the below code blocks. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "MOMhmIlGprK9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Train Data ===========\n",
      "                                                   0        1\n",
      "0                            i didnt feel humiliated  sadness\n",
      "1  i can go from feeling so hopeless to so damned...  sadness\n",
      "2   im grabbing a minute to post i feel greedy wrong    anger\n",
      "3  i am ever feeling nostalgic about the fireplac...     love\n",
      "4                               i am feeling grouchy    anger\n",
      "=========== Validation Data ===========\n",
      "                                                   0        1\n",
      "0  im feeling rather rotten so im not very ambiti...  sadness\n",
      "1          im updating my blog because i feel shitty  sadness\n",
      "2  i never make her separate from me because i do...  sadness\n",
      "3  i left with my bouquet of red and yellow tulip...      joy\n",
      "4    i was feeling a little vain when i did this one  sadness\n",
      "=========== ===========\n",
      "Train Shape: (16000, 2)\n",
      "Validation Shape: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the train.txt and val.txt file into pandas dataframe format \n",
    "train = pd.read_csv('Emotions-dataset/train.txt', sep=\";\", header=None)\n",
    "test = pd.read_csv('Emotions-dataset/test.txt', sep=\";\", header=None)\n",
    "\n",
    "# train \n",
    "print(\"=========== Train Data ===========\")\n",
    "print(train.head())\n",
    "\n",
    "# validation\n",
    "print(\"=========== Validation Data ===========\")\n",
    "print(test.head())\n",
    "print(\"=========== ===========\")\n",
    "\n",
    "# and printout the train.shape and validation.shape \n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Validation Shape:\", test.shape)\n",
    "\n",
    "# expected shape of train dataset is (16000,2) and validation dataset is (2000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "PS7K3p7AqAI8"
   },
   "outputs": [],
   "source": [
    "# clean the text in the train and validation dataframes using the clean_text function provided above\n",
    "for train_loc in range(train.shape[0]):\n",
    "    train.iloc[train_loc][0] = clean_text(train.iloc[train_loc][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_loc in range(test.shape[0]):\n",
    "    test.iloc[test_loc][0] = clean_text(test.iloc[test_loc][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "GxZC6RIjq3bu"
   },
   "outputs": [],
   "source": [
    "# initialise count vectorizer from sklearn module with default parameter\n",
    "count_vec = CountVectorizer()\n",
    "\n",
    "# fit on train dataset and transform both train and validation dataset\n",
    "fit_count_train_x = count_vec.fit(list(train[0]), list(train[1]))\n",
    "\n",
    "trans_count_train_x = fit_count_train_x.transform(list(train[0]))\n",
    "trans_count_test_x = fit_count_train_x.transform(list(test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "85nuazEzq3YT"
   },
   "outputs": [],
   "source": [
    "# initialise tfidf vectorizer from sklearn module with default parameter\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# fit on train dataset and transform both train and validation dataset\n",
    "fit_tfidf_train_x = tfidf_vec.fit(list(train[0]), list(train[1]))\n",
    "\n",
    "trans_count_train_x = fit_tfidf_train_x.transform(list(train[0]))\n",
    "trans_count_test_x = fit_tfidf_train_x.transform(list(test[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "jgHWAuG-q3Vm"
   },
   "outputs": [],
   "source": [
    "# initialise label encoder from sklearn module\n",
    "label_vec = preprocessing.LabelEncoder()\n",
    "\n",
    "# fit on train labels and transform both train and validation labels\n",
    "fit_label_train_x = label_vec.fit(list(train[1]))\n",
    "\n",
    "trans_label_train_x = fit_label_train_x.transform(list(train[1]))\n",
    "trans_label_test_x = fit_label_train_x.transform(list(test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjdye0tvq3So"
   },
   "outputs": [],
   "source": [
    "# convert the labels into one hot encoding form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjsiH8YOw-Da"
   },
   "source": [
    "### Q) Build the neural networks using tensorflow keras by following the below instructions. Evaluate the model on different metrics and comment your observations. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQg14bkTq3KB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# complete this linear model in tensorflow\n",
    "def build_model(X):\n",
    "\n",
    "  # layer 1 : input layer\n",
    "    inp = tf.keras.Input((X.shape[1],))\n",
    "\n",
    "  # layer 2 : add the dense layer with 2048 units and relu activation\n",
    "    x = layers.Dense(2048, activation='relu')(inp)\n",
    "    \n",
    "  # layer 3 : add the dropout layer with dropout rate of 0.5\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "  \n",
    "  # layer 4 : add the dense layer with 1024 units with tanh activation and with l2 regularization\n",
    "    x = layers.Dense(1024, activation='tanh', kernel_regularizer=regularizers.L2(1e-4))(inp)\n",
    "\n",
    "  # layer 5 : add the dropout layer with dropout rate of 0.5\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "  # layer 6 : add the dense layer with 512 units with tanh activation and with l2 regularization\n",
    "    x = layers.Dense(512, activation='tanh', kernel_regularizer=regularizers.L2(1e-4))(inp)\n",
    "\n",
    "  # layer 7 : add the dropout layer with dropout rate of 0.5\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "  # layer 8 : add the dense layer with 256 units with tanh activation and with l2 regularization\n",
    "    x = layers.Dense(256, activation='tanh', kernel_regularizer=regularizers.L2(1e-4))(inp)\n",
    "\n",
    "  # layer 9 : add the dropout layer with dropout rate of 0.5\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "  # layer 10 : add the dense layer with 128 units with tanh activation and with l2 regularization\n",
    "    x = layers.Dense(128, activation='tanh', kernel_regularizer=regularizers.L2(1e-4))(inp)\n",
    "\n",
    "  # layer 11 : add the dropout layer with dropout rate of 0.5\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "  # layer 12 : output layer with units equal to number of classes and activation as softmax\n",
    "    x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "  # use loss as categorical crossentropy, optimizer as rmsprop and evaluate model on auc,precision,recall,accuracy \n",
    "    model = tf.keras.Model(inp, x)\n",
    "    \n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop', metrics = [auc, precision, recall, 'accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q71CC1pIsx0O"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p4/yl_ws7ds50z7czgwp8yk_hl40000gn/T/ipykernel_26226/2135244932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# call the build_model function and initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "# call the build_model function and initialize the model\n",
    "model = build_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyAZNgsBsxwo"
   },
   "outputs": [],
   "source": [
    "# train and validate the model on the count vectors of text which we have created initially for 10 epochs, \n",
    "# adjust batch size according to your computation power (suggestion use : 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nS02IwLCsxmG"
   },
   "outputs": [],
   "source": [
    "# plot train loss vs val loss, train auc vs val auc, train recall vs val recall, train precision vs val precision and train accuracy vs val accuracy and comment your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FHdCcp7wXyw"
   },
   "outputs": [],
   "source": [
    "# again call the build_model function and initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4gB80M6wXvV"
   },
   "outputs": [],
   "source": [
    "# train and validate the model on the tfidf vectors of text which we have created initially for 10 epochs, \n",
    "# adjust batch size according to your computation power (suggestion use : 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsnEOHXRwXkv"
   },
   "outputs": [],
   "source": [
    "# plot train loss vs val loss, train auc vs val auc, train recall vs val recall, train precision vs val precision and train accuracy vs val accuracy and comment your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4adDDQWeQyGb"
   },
   "source": [
    "## Question 4 Theory Question  \n",
    "\n",
    "What is the difference between Count Vectorizer, TFIDF, Word2Vec and Glove? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXTRkF6KRB_D"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating word vectors, both unsupervised models - Glove and Word2vec can be used. Mechanism of generating word vectors is the main difference between them. The word vectors generated by either of these models can be used for a wide variety of tasks ranging such as finding words that are semantically similar to a word, representing a word when it is being input to a downstream model. A word embedding representation of a word captures more information about a word than just a one-hot representation of the word, since the former captures semantic similarity of that word to other words whereas the latter representation of the word is equidistant from all other words. Tf-idf is a scoring scheme for words - that is a measure of how important a word is to a document.\n",
    "\n",
    "In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently. And to overcome this, we use TfidfVectorizer .\n",
    "\n",
    "In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.\n",
    "\n",
    "From a practical usage standpoint, while tf-idf is a simple scoring scheme and that is its key advantage, word embeddings may be a better choice for most tasks where tf-idf is used, particularly when the task can benefit from the semantic similarity captured by word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQQsqcto79zE"
   },
   "source": [
    "What is the significant difference between the Niave Bayes Implementation using Bag of Words and TF-IDF? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtiPCTZM8Aua"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaXBo1Bw8C1K"
   },
   "source": [
    "As we know Bag of Words just creates a set of vectors containing the count of word occurrences in the document, while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "\n",
    "The tf-idf is an statistic that increases with the number of times a word appears in the document, penalized by the number of documents in the corpus that contain the word. Using method - sklearn.feature_extraction.text, we can achieve this.\n",
    "\n",
    "Where as the term “bag of words” is widely used as the selected document to be processed under the context of Naive Bayes as well as while depicting the document itself as a bag. Also, each vocabulary in the texture as the items in the bag by permitting multiple occurrences.\n",
    "\n",
    "The significant advantage of using TF-IDF is that it enables us to gives us a way to associate each word in a document with a number that represents how relevant each word is in that document. Then, documents with similar, relevant words will have similar vectors, which is what we are looking for in a machine learning algorithm."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS6120_NLP_Assignment_2_Notebook",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
